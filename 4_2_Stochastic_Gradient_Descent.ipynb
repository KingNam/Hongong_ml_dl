{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_2 Stochastic Gradient Descent",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdw0eQBguqiQpUQ0bHDqyS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingNam/Hongong_ml_dl/blob/main/4_2_Stochastic_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TChqCp8yLOJ0"
      },
      "source": [
        "# 확률적 경사 하강법\n",
        "\n",
        "점진적학습?\n",
        "- 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 학습시키는 방법\n",
        "\n",
        "점진적 학습 알고리즘 중 가장 대표적인 알고리즘을 **확률적 경사 하강법** 이라 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoosJAxNN6Qc"
      },
      "source": [
        "### 확률적 경사 하강법 (Stochastic Gradient Descent)\n",
        "\n",
        " - Stochastic: 무작위 확률 분산을 가지거나 통계적으로 분석 가능한 패턴이나 정확히 예측할 수 없는(\n",
        " randomly determined; having a random probability distribution or pattern that may be analyzed statistically but may not be predicted precisely.)\n",
        "\n",
        " - Gradient: 경사(an inclined part of a road or railway; a slope)\n",
        "\n",
        " - Descent: 하강\n",
        "\n",
        "> 즉, 확률적 경사 하강법이란 무작위하게 경사를 따라 내려가는 방법\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWeK5t19rYwz"
      },
      "source": [
        "확률적 경사 하강법을 배우기 전에\n",
        "\n",
        "1. 경사에 대해서 알아보고\n",
        "\n",
        "2. 경사 하강법에 대해 알아보자\n",
        "\n",
        "[관련링크(공돌이의 수학정리노트)](https://angeloyeo.github.io/2019/08/25/gradient.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGjUXSQrrj8P"
      },
      "source": [
        "### 1.1 편미분 (partial derivative)\n",
        "\n",
        "다변수 함수의 특정 변수를 제외한 나머지 변수를 상수로 간주하여 미분하는 것(위키피디아)\n",
        "\n",
        "그렇다면, 편미분은 어떤 상황에서 필요한가?\n",
        "\n",
        "독립변수가 2개이고 종속변수가 하나인 단순한형태까지만 확장한 다변량 함수를 보자\n",
        "\n",
        "ex. $f(x, y) = x^{2} + xy + y^{2}$\n",
        "\n",
        "![](https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-08-25_gradient/noname01.png)\n",
        "\n",
        "이 함수에서 하나의 점에 대한 기울기를 구하는 것이 가능한가? \n",
        "-  유일한 직선으로 결정되지 않는다.\n",
        "- 즉, $x$ 방향 만으로의 기울기와 $y$ 방향 만으로의 기울기는 각각 구할 수 있기 때문에 편미분을 통해 $x$ or $y$ 방향만으로의 기울기를 구한다.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaoblbGFJ7CJ"
      },
      "source": [
        "### 1.2 Gradient(기울기)\n",
        "\n",
        "$x$ 방향으로 편미분 값과 $y$ 방향으로의 편미분 값을 원소로 하는 벡터를 출력한다\n",
        "\n",
        "이것을 수식으로 표현하면 다음과 같다.\n",
        "\n",
        "$gradient(f) = f_{x}\\hat{i} + f_{y}\\hat{j} = \n",
        "\\frac{\\partial}{\\partial{x}}f(x,y)\\hat{i} + \\frac{\\partial}{\\partial{y}}f(x,y)\\hat{j} $  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOfMRNFdPgUt"
      },
      "source": [
        "또는, \n",
        "\n",
        "$\\nabla{f} = f_{x}\\hat{i} + f_{y}\\hat{j} = \n",
        "\\frac{\\partial}{\\partial{x}}f(x,y)\\hat{i} + \\frac{\\partial}{\\partial{y}}f(x,y)\\hat{j} $  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS59GOtkPirE"
      },
      "source": [
        "그럼 위의 식 $f(x, y) = x^{2} + xy + y^{2}$\n",
        "을 $x$와 $y$에 대해 편미분 하자\n",
        "\n",
        "$\\frac{\\partial}{\\partial{x}}f(x,y) = \\frac{\\partial}{\\partial{x}}(x^{2} + xy + y^{2}) = 2x+y $\n",
        "\n",
        "$\\frac{\\partial}{\\partial{y}}f(x,y) = \\frac{\\partial}{\\partial{x}}(x^{2} + xy + y^{2}) = 2y+x $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh_OJZINQHmn"
      },
      "source": [
        "그러면, scalar로 표현되어 있는 함수에서 벡터 값을 얻어낼 수 있다.\n",
        "\n",
        "이 그래디언트(두개의 벡터)를 통해 함수의 해당 점이 어느 방향으로 값이 커지고 있는지 나타낼 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG9CfwHjQxrK"
      },
      "source": [
        "### 2. 경사하강법 (gradient descent)\n",
        "\n",
        "1) 함수 값이 **낮아지는 방향**으로 독립 변수 값을 변형시키면서 최종적으로는 최소 함수 값을 갖도록 하는 독립 변수 값을 찾는 방법이다.\n",
        "\n",
        "2) 함수의 기울기(gradient)를 이용해 $x$의 값을 어디로 옮겼을 때 함수가 최소값을 찾는지 알아보는 방법\n",
        "\n",
        "- 기울기가 양수라는 것은 $x$값이 커질 수록 함수 값이 커진다는 것을 의미\n",
        "- 기울기가 음수라는 것은 $x$ 값이 커질 수록 함수의 값이 작아진다는 것을 의미\n",
        "- 또, 기울기의 값이 크다는 것은 가파르다는 것을 의미하기도 하지만, 한편으로는 $x$의 위치가 최소값/최대값에 해당하는 좌표로부터 멀리 떨어져있다는것을 의미"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK3p_lUmRKwN"
      },
      "source": [
        "이를 이용하면 임의의 점 $x$가 커질 수록 함수값이 커지는 중이라면 음의방향으로 $x$를 옮겨야 할 것이고, 반대로 특정포인트 $x$ 에서 $x$가 커질 수록 함수값이 작아지는 중이라면 양의 방향으로 $x$를 옮기면 된다.\n",
        "\n",
        "그러면 수식은 다음과 같다.\n",
        "\n",
        "$x_{i+1} = x_{i} - (이동거리) \\times(기울기의 부호)$\n",
        "\n",
        "여기서 이동거리는 사용자가 적절히 조절할수있게 수식을 조정한다.\n",
        "\n",
        "따라서, 최종수식은 다음과 같다.\n",
        "\n",
        "$x_{i+1} = x_{i} - \\alpha\\frac{\\partial{f}}{\\partial{x}}x_{i} $\n",
        "\n",
        "이름 다변수함수에 대해 확정하면 다음과 같이 쓸 수 있다.\n",
        "\n",
        "$x_{i+1} = x_{i} - \\alpha\\nabla f(x_{i}) $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alcbwu7-jsgl"
      },
      "source": [
        "### 3. 확률적 경사 하강법 (Stochastic Gradient Descent)\n",
        "훈련세트에서 랜덤하게 하나의 샘플을 선택하여 가파른 경사를 조금 내려간다. 그다음 훈련 세트에서 랜덤하게 또 다른 샘플을 하나 선택하여 경사를 조금 내려간다. 이런 식으로 전체 샘플을 모두 사용할 때까지 계속한다.\n",
        "\n",
        "만약 모든 샘플을 다썼는데도 원하는 위치에 도달하지 못했다면?\n",
        "\n",
        "위 과정을 반복적으로 진행한다. \n",
        "\n",
        "이렇게 SGD에서 훈련세트를 한 번 모두 사용하는 과정을 **에포크(epoch)** 라고 부른다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oan5i8trqVN7"
      },
      "source": [
        "훈련과정과 사용되는 샘플에 따라 다음과 같은 순서를 따른다.\n",
        "\n",
        "1. 훈련세트에서 샘플을 꺼낸다\n",
        " - 1개씩 꺼내면 **확률적 경사 하강법(SGD)**\n",
        " - 여러개씩 꺼내면 **미니배치 경사 하강법(Minibatch GD)**\n",
        " - 한번에 꺼내면 **배치 경사 하강법(Batch GD)**\n",
        "\n",
        " \n",
        "2. 샘플을 이용하여 경사를 이동함 (반복)\n",
        "\n",
        "3. 훈련세트를 다 사용했나요?\n",
        "  - Yes, 1 에포크 완료\n",
        "  - No, 1을 반복\n",
        "\n",
        "4. 훈련세트에 샘플을 모두 채우고 다시 시작 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHIgOa-CBbkW"
      },
      "source": [
        "### 4. 손실 함수 (Loss function)\n",
        "\n",
        "어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지 측정하는 기준\n",
        "\n",
        "손실 함수의 값이 작을 수록 좋다.\n",
        "\n",
        "그러나 어떤 값이 최소값인지 알지 못하고 가능한 많이 찾아보고 결정\n",
        "\n",
        "따라서, 값을 효율적으로 계산할 수 있는 SGD 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiMBMfmcCasw"
      },
      "source": [
        "1) 로지스틱 손실 함수 (이진 크로스엔트로피 손실함수)\n",
        "\n",
        "로지스틱 회귀에서 사용하는 손실함수는 두가지 결과를 내는 이진 (binary) 함수를 사용한다.\n",
        "\n",
        "그 전에, 크로스엔트로피 (Cross-Entropy) 에 대해 이해해보자. \n",
        "\n",
        "그러기 위해선 다음에 대한 이해가 필요하다.\n",
        " > 정보량 / 엔트로피 / 크로스 엔트로피\n",
        "\n",
        "- 정보량 (Information)\n",
        "[출처](http://www.aistudy.com/control/information_theory.htm)\n",
        "\n",
        " 한줄요약: \"호의가 계속되면, 그게 권리인줄 알아요\" \n",
        "[동영상링크]( https://www.youtube.com/watch?v=n4Hg-dQNP-E)\n",
        "\n",
        " Claude Shannon이 확립한 정보이론은 **정보를 운반하고 있는 기호의 확률적 성질에 착안한 것**이다.\n",
        "\n",
        " 여기서 말하는 정보란, 신문의 뉴스나 특종기사와 같은 정보가 아니라, 문자,음성,유전자 등 넓은 의미의 정보를 말한다.\n",
        "\n",
        " 샤논은 1948년 \"통신의 수학적 이론(A Mathmatical Theory of Communication)\" 이라는 논문을 발표함으로써, \"정보량(Information Content)\"의 정의를 세우고 여러 가지 문제를 해결할 수 있는 이론적 기초를 제공했다.\n",
        "\n",
        " 샤논은 정보단위로 2진법 or 비트를 택했다.\n",
        " 이것은 단지 2개의 부호 1(Yes) or 0(No)로 아주 간편하고 효율적인 코드이기 때문이다.\n",
        " \n",
        " 다음 1~16 숫자맞추기 게임을 생각해보자. \n",
        " \n",
        " 정답은 9라고 가정하면 다음에 질문을 통해 답을 맞출 수 있다.\n",
        "\n",
        " \"8이하입니까?\"  --> \"아니오\"\n",
        "\n",
        " \"12이하입니까?   --> \"예\"\n",
        "\n",
        " \"10이하입니까?\" -->  \"예\"\n",
        "\n",
        " \"9이하입니까? --> \"예\"\n",
        "\n",
        " 정답은 9네요!\n",
        "\n",
        " 여기서 답을 알기위해서는 다음 세가지 조건이 필요하다.\n",
        "\n",
        " 첫째, 숫자의 수. 여기서는 16개 이므로 아무 정보도 없는 상황에서 숫자를 맞출 확률은 $\\frac{1}{16}$이다.\n",
        "\n",
        " 둘째, 대답의 종류는 두가지(예, 아니오) 로 허용된다.\n",
        "\n",
        " 셋째, 질문횟수. 16개의 수 중 특정 수를 알아맞히는데는 4회의 질문이 필요하다.\n",
        "\n",
        " 이 세가지 조건은 다음과 같은 수식으로 표현된다.\n",
        "\n",
        " $2^4 = 16$ \n",
        "\n",
        " 이 식을 일반화시키면 다음과 같다.\n",
        "\n",
        " $ W = 2^n $\n",
        " \n",
        " 여기서, $W$는 최대숫자 이고 $n$은 질문횟수다. 위 식의 양변에 로그를 취하면\n",
        "\n",
        " $ log{W} = nlog{2}$  ==>  $n = log_{2}^{W}$  이다.\n",
        "\n",
        " 여기서 $W$대신 어떤 사상의 확률 $p$라고 하면\n",
        "\n",
        " $n = -log_{2}^{P}$ 로 정의된다.\n",
        "\n",
        " 더 자세한 공부는 출처를 통해 공부하자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0byJFTHu5fZA"
      },
      "source": [
        "- Entropy\n",
        " \n",
        " 엔트로피는 정보량의 기댓값을 말한다. 따라서, 기댓값의 정의에 따라 아래와 같다.\n",
        "\n",
        " $Entropy = E[-log^{p(x)}] = \\sum_{x} -p(x)log^{p(x)}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZQSOY8O5gt8"
      },
      "source": [
        "- Cross Entropy\n",
        "\n",
        " 다른 확률을 곱해서 Entropy를 계산한 것.\n",
        "\n",
        " 예를 들어, 0 또는 1만 가지는 확률변수 $X$에 대해서\n",
        "\n",
        " $entropy = -p(X=0)log(p(X=0)-p(X=1)log(p(X=1)$\n",
        "\n",
        " $cross-entropy = -p(X=0)log(p(X=0)-p(X=1)log(p(X=1)$\n",
        "\n",
        " 이고, binary에서 확장해 본다면\n",
        "\n",
        " $CE(p,q) = - \\sum p(x) \\log q(x)$ 이다. \n",
        "\n",
        " 분류문제에서는 위 식을 손실함수로 사용하고 값이 작을수록 좋다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M86kLNaOm6cm"
      },
      "source": [
        "### 5. SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg8nER6UnEHW"
      },
      "source": [
        "import pandas as pd \n",
        "fish = pd.read_csv(\"https://bit.ly/fish_csv_data\")\n",
        "fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()\n",
        "fish_target = fish['Species'].to_numpy()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21YfO92YoH2U"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_input, test_input, train_target, test_target = train_test_split(\n",
        "    fish_input, fish_target, random_state = 42\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdnTM0EPoldF"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "ss.fit(train_input)\n",
        "train_scaled = ss.transform(train_input)\n",
        "test_scaled = ss.transform(test_input)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BjHcJT_ozM6"
      },
      "source": [
        "#확률적경사하강법을 제공하는 대표적인 분류용클래스는 SGDClassifer\n",
        "from sklearn.linear_model import SGDClassifier"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cajYdLVvpS4S",
        "outputId": "28e7b1bf-8d19-46c7-c0a9-704d360402dd"
      },
      "source": [
        "sc = SGDClassifier(loss = 'log', max_iter = 10, random_state =42)\n",
        "sc.fit(train_scaled,train_target)\n",
        "print(sc.score(train_scaled,train_target))\n",
        "print(sc.score(test_scaled, test_target))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.773109243697479\n",
            "0.775\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UP_rUenpad5",
        "outputId": "fe1dc64c-e4ca-4ee9-8608-499fcc727a6e"
      },
      "source": [
        "# 모델을 이어서 훈련할때는 partial_fit() 메서드를 사용함\n",
        "# 1에포크씩 이어서 훈련된다.\n",
        "sc.partial_fit(train_scaled, train_target)\n",
        "print(sc.score(train_scaled, train_target))\n",
        "print(sc.score(test_scaled, test_target))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8739495798319328\n",
            "0.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7bDyV1lpt0y"
      },
      "source": [
        "그렇다면, 얼마나 더 훈련해야할까?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuqB28FJrNwL"
      },
      "source": [
        "### 6. 에포크와 과대/과소적합\n",
        "\n",
        "SGD를 사용한 모델은 에포크 횟수에 따라 과소적합이나 과대적합문제가 나올 수 있다.\n",
        "\n",
        "에포크 횟수가 적으면 모델이 훈련 세트를 덜 학습한다. 분포를 전부 보지 못하고 훈련을 마치는 셈. 에포크 횟수가 많으면 훈련세트를 완전히 학습한다. 훈련 세트에 아주 잘 맞는 모델이 만들어진다.\n",
        "\n",
        "즉, 적은 에포크횟수로 훈련한 모델은 과소적합 된 모델을 가능성이 있고 많은 에포크횟수로 훈련한 모델은 과대적합된 모델일 가능성이 있다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoyrlXvCrRtx"
      },
      "source": [
        "# 에포크에 따른 정확도 그래프를 통해 적절한 에포크 수 찾기\n",
        "\n",
        "import numpy as np\n",
        "sc = SGDClassifier(loss='log', random_state= 42)\n",
        "train_score =[]\n",
        "test_score = []\n",
        "classes = np.unique(train_target)\n",
        "\n",
        "for _ in range(0,300):\n",
        "  sc.partial_fit(train_scaled, train_target, classes = classes)\n",
        "  train_score.append(sc.score(train_scaled, train_target))\n",
        "  test_score.append(sc.score(test_scaled, test_target))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "6_oKgwLdtB3E",
        "outputId": "933162ac-1cae-46bb-af48-253392d27111"
      },
      "source": [
        "# 300번의 에포크 똥안 기록한 훈련 세트와 테스트 세트의 점수를 그래포로 그려 보자.\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_score)\n",
        "plt.plot(test_score)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2DtV2B6tW00",
        "outputId": "d2e5b820-88f7-462d-a9b4-c1e3760fcff2"
      },
      "source": [
        "# SGDClassifer 는 일정 에포크동안 성능이 향상되지 않으면 더 훈련하지 않고 자동으로 멈춤\n",
        "# tol 매개변수에서 향상될 최솟값을 지정함.\n",
        "# 아래의 코드에서는 향상될 최솟값을 None으로 두고 진행.\n",
        "\n",
        "sc100 = SGDClassifier(loss='log',max_iter=100,tol=None, random_state=42)\n",
        "sc100.fit(train_scaled, train_target)\n",
        "print(sc100.score(train_scaled, train_target))\n",
        "print(sc100.score(test_scaled, test_target))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.957983193277311\n",
            "0.925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zCwI3UWwB92"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}