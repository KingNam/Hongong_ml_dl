{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_2 Stochastic Gradient Descent",
      "provenance": [],
      "authorship_tag": "ABX9TyOk+81X8yTYY6KW/9bOYGSv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingNam/Hongong_ml_dl/blob/main/4_2_Stochastic_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TChqCp8yLOJ0"
      },
      "source": [
        "# 확률적 경사 하강법\n",
        "\n",
        "점진적학습?\n",
        "- 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 학습시키는 방법\n",
        "\n",
        "점진적 학습 알고리즘 중 가장 대표적인 알고리즘을 **확률적 경사 하강법** 이라 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoosJAxNN6Qc"
      },
      "source": [
        "### 확률적 경사 하강법 (Stochastic Gradient Descent)\n",
        "\n",
        " - Stochastic: 무작위 확률 분산을 가지거나 통계적으로 분석 가능한 패턴이나 정확히 예측할 수 없는(\n",
        " randomly determined; having a random probability distribution or pattern that may be analyzed statistically but may not be predicted precisely.)\n",
        "\n",
        " - Gradient: 경사(an inclined part of a road or railway; a slope)\n",
        "\n",
        " - Descent: 하강\n",
        "\n",
        "> 즉, 확률적 경사 하강법이란 무작위하게 경사를 따라 내려가는 방법\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWeK5t19rYwz"
      },
      "source": [
        "확률적 경사 하강법을 배우기 전에\n",
        "\n",
        "1. 경사에 대해서 알아보고\n",
        "\n",
        "2. 경사 하강법에 대해 알아보자\n",
        "\n",
        "[관련링크(공돌이의 수학정리노트)](https://angeloyeo.github.io/2019/08/25/gradient.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGjUXSQrrj8P"
      },
      "source": [
        "### 1.1 편미분 (partial derivative)\n",
        "\n",
        "다변수 함수의 특정 변수를 제외한 나머지 변수를 상수로 간주하여 미분하는 것(위키피디아)\n",
        "\n",
        "그렇다면, 편미분은 어떤 상황에서 필요한가?\n",
        "\n",
        "독립변수가 2개이고 종속변수가 하나인 단순한형태까지만 확장한 다변량 함수를 보자\n",
        "\n",
        "ex. $f(x, y) = x^{2} + xy + y^{2}$\n",
        "\n",
        "![](https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-08-25_gradient/noname01.png)\n",
        "\n",
        "이 함수에서 하나의 점에 대한 기울기를 구하는 것이 가능한가? \n",
        "-  유일한 직선으로 결정되지 않는다.\n",
        "- 즉, $x$ 방향 만으로의 기울기와 $y$ 방향 만으로의 기울기는 각각 구할 수 있기 때문에 편미분을 통해 $x$ or $y$ 방향만으로의 기울기를 구한다.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaoblbGFJ7CJ"
      },
      "source": [
        "### 1.2 Gradient(기울기)\n",
        "\n",
        "$x$ 방향으로 편미분 값과 $y$ 방향으로의 편미분 값을 원소로 하는 벡터를 출력한다\n",
        "\n",
        "이것을 수식으로 표현하면 다음과 같다.\n",
        "\n",
        "$gradient(f) = f_{x}\\hat{i} + f_{y}\\hat{j} = \n",
        "\\frac{\\partial}{\\partial{x}}f(x,y)\\hat{i} + \\frac{\\partial}{\\partial{y}}f(x,y)\\hat{j} $  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOfMRNFdPgUt"
      },
      "source": [
        "또는, \n",
        "\n",
        "$\\nabla{f} = f_{x}\\hat{i} + f_{y}\\hat{j} = \n",
        "\\frac{\\partial}{\\partial{x}}f(x,y)\\hat{i} + \\frac{\\partial}{\\partial{y}}f(x,y)\\hat{j} $  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS59GOtkPirE"
      },
      "source": [
        "그럼 위의 식 $f(x, y) = x^{2} + xy + y^{2}$\n",
        "을 $x$와 $y$에 대해 편미분 하자\n",
        "\n",
        "$\\frac{\\partial}{\\partial{x}}f(x,y) = \\frac{\\partial}{\\partial{x}}(x^{2} + xy + y^{2}) = 2x+y $\n",
        "\n",
        "$\\frac{\\partial}{\\partial{y}}f(x,y) = \\frac{\\partial}{\\partial{x}}(x^{2} + xy + y^{2}) = 2y+x $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh_OJZINQHmn"
      },
      "source": [
        "그러면, scalar로 표현되어 있는 함수에서 벡터 값을 얻어낼 수 있다.\n",
        "\n",
        "이 그래디언트(두개의 벡터)를 통해 함수의 해당 점이 어느 방향으로 값이 커지고 있는지 나타낼 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG9CfwHjQxrK"
      },
      "source": [
        "### 2. 경사하강법 (gradient descent)\n",
        "\n",
        "1) 함수 값이 **낮아지는 방향**으로 독립 변수 값을 변형시키면서 최종적으로는 최소 함수 값을 갖도록 하는 독립 변수 값을 찾는 방법이다.\n",
        "\n",
        "2) 함수의 기울기(gradient)를 이용해 $x$의 값을 어디로 옮겼을 때 함수가 최소값을 찾는지 알아보는 방법\n",
        "\n",
        "- 기울기가 양수라는 것은 $x$값이 커질 수록 함수 값이 커진다는 것을 의미\n",
        "- 기울기가 음수라는 것은 $x$ 값이 커질 수록 함수의 값이 작아진다는 것을 의미\n",
        "- 또, 기울기의 값이 크다는 것은 가파르다는 것을 의미하기도 하지만, 한편으로는 $x$의 위치가 최소값/최대값에 해당하는 좌표로부터 멀리 떨어져있다는것을 의미"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK3p_lUmRKwN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}